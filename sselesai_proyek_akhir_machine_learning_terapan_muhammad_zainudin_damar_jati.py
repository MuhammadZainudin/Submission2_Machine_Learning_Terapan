# -*- coding: utf-8 -*-
"""SSelesai_Proyek_Akhir_Machine_Learning_Terapan_Muhammad_Zainudin_Damar_Jati.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rUWdKakEZbd_4x0HgWa6b48HDcoD79Iw

#Proyek Machine Learning Recommender System - Muhammad Zainudin Damar Jati

## Project Overview

Sistem rekomendasi telah menjadi komponen penting dalam banyak platform digital modern, termasuk dalam industri hiburan seperti streaming anime. Seiring pertumbuhan industri anime dan diversifikasi preferensi penonton, kebutuhan akan sistem yang dapat memberikan rekomendasi personalisasi semakin mendesak. Sistem rekomendasi memungkinkan pengguna untuk menemukan konten baru yang relevan berdasarkan preferensi mereka, sehingga meningkatkan keterlibatan pengguna dan durasi penggunaan platform.

Masalah ini penting diselesaikan karena jumlah anime yang sangat banyak bisa membuat pengguna kesulitan menentukan pilihan tontonan. Pendekatan tradisional seperti pencarian manual atau daftar populer tidak mampu memenuhi kebutuhan personalisasi pengguna secara optimal.

Berdasarkan studi oleh Bobadilla et al. (2013), sistem rekomendasi dapat dikategorikan menjadi Collaborative Filtering, Content-Based Filtering, dan Hybrid Systems, masing-masing dengan kelebihan dan keterbatasan tertentu \[1].

> \[1] J. Bobadilla, F. Ortega, A. Hernando, and A. Gutiérrez, “Recommender systems survey,” *Knowledge-Based Systems*, vol. 46, pp. 109–132, 2013.

## Business Understanding

### Problem Statements

* Bagaimana memberikan rekomendasi anime yang relevan berdasarkan genre?
* Bagaimana memprediksi preferensi pengguna baru atau lama berdasarkan pola rating pengguna lain?

### Goals

* Menghasilkan daftar rekomendasi anime berdasarkan genre yang mirip dengan anime favorit pengguna.
* Memprediksi rating atau minat pengguna terhadap anime yang belum ditonton, berdasarkan perilaku kolektif pengguna lain.

### Solution Statements

* **Content-Based Filtering (CBF)** : Sistem rekomendasi berdasarkan kemiripan item. Fitur utama yang digunakan adalah genre anime. Pendekatan ini sangat efektif untuk pengguna baru (cold-start) karena hanya mengandalkan informasi item.
* **Neural Collaborative Filtering (NCF)** : Menggunakan pendekatan berbasis deep learning untuk mempelajari interaksi antara pengguna dan item. Model ini mampu menangkap hubungan non-linear yang kompleks dalam preferensi pengguna.

## Data Understanding

### Import Library
Kode ini mengimpor berbagai pustaka (library) yang digunakan untuk pengolahan data, visualisasi, pembelajaran mesin, dan pembangunan model deep learning.
"""

import os
import pickle
import random
import zipfile

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from google.colab import files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
)
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.utils import shuffle
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import (
    BatchNormalization,
    Concatenate,
    Dense,
    Dropout,
    Embedding,
    Flatten,
    Input,
    LeakyReLU,
)
from tensorflow.keras.losses import Huber
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

"""Library Sistem dan Utilitas
* `os` : Untuk operasi sistem file, seperti membaca atau menulis file dan direktori.
* `pickle` : Untuk menyimpan dan memuat objek Python secara efisien (serialisasi).
* `random` : Untuk operasi terkait bilangan acak.
* `zipfile` : Untuk membuka dan mengekstrak file arsip `.zip`.

Visualisasi
* `matplotlib.pyplot` : Library utama untuk membuat grafik dan plot.
* `seaborn` : Library berbasis matplotlib untuk visualisasi statistik yang lebih menarik dan mudah.

Manipulasi dan Analisis Data
* `numpy` : Library utama untuk operasi numerik dan array multidimensi.
* `pandas` : Library untuk manipulasi data berbasis tabel (DataFrame).

Google Colab Utilities
* `google.colab.files` : Modul khusus untuk mengupload dan mendownload file dalam lingkungan Google Colab.

Scikit-learn (sklearn) untuk Machine Learning
* `TfidfVectorizer` : Untuk mengubah teks menjadi fitur TF-IDF (untuk pengolahan teks).
* `mean_absolute_error`, `mean_squared_error` : Fungsi evaluasi error untuk model regresi.
* `cosine_similarity` : Menghitung kemiripan kosinus antar vektor.
* `train_test_split` : Membagi dataset menjadi data pelatihan dan pengujian.
* `LabelEncoder` : Mengubah label kategorikal menjadi angka.
* `MinMaxScaler` : Normalisasi fitur agar berada dalam rentang tertentu (misal 0–1).
* `shuffle` : Mengacak urutan data.

TensorFlow dan Keras untuk Deep Learning
* `tensorflow` : Framework utama untuk machine learning dan deep learning.
* `tensorflow.keras.callbacks` : Fungsi untuk menghentikan pelatihan lebih awal (EarlyStopping) dan mengatur learning rate saat pelatihan (ReduceLROnPlateau).
* `tensorflow.keras.layers` : Berbagai lapisan jaringan saraf (Dense, Embedding, Dropout, dll) yang digunakan untuk membangun model.
* `tensorflow.keras.losses.Huber` : Fungsi loss Huber, yang menggabungkan MSE dan MAE untuk regresi.
* `tensorflow.keras.models.Model` : API Keras untuk membuat model fungsional.
* `tensorflow.keras.optimizers.Adam` : Optimizer Adam untuk pembaruan bobot model.
* `tensorflow.keras.regularizers.l2` : Regularisasi L2 untuk mencegah overfitting.

### Load & Explore Data

Kode ini digunakan untuk mengunggah file dari komputer lokal ke Google Colab, lalu memeriksa setiap file yang diunggah; jika file tersebut berformat `.zip`, maka sistem akan secara otomatis membuat folder dengan nama yang sama (tanpa ekstensi `.zip`), kemudian mengekstrak seluruh isi file ZIP tersebut ke dalam folder tersebut menggunakan modul `zipfile`, serta menampilkan pesan konfirmasi bahwa proses ekstraksi berhasil, sementara jika file yang diunggah bukan file ZIP, maka akan ditampilkan pesan bahwa file tersebut tidak diproses.
"""

uploaded = files.upload()

for filename in uploaded.keys():
    if filename.endswith(".zip"):
        zip_path = filename
        extract_dir = os.path.splitext(zip_path)[0]
        os.makedirs(extract_dir, exist_ok=True)

        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        print(f"File '{zip_path}' berhasil diekstrak ke folder '{extract_dir}'")
    else:
        print(f"File '{filename}' bukan file ZIP")

"""Kode ini dilakuakan pmembacaan dua file CSV bernama anime.csv dan rating.csv yang berada dalam folder bernama 'Anime Recommendations Database', lalu masing-masing file dimuat ke dalam objek DataFrame anime dan rating menggunakan fungsi read_csv dari library pandas, yang memungkinkan data dianalisis dan dimanipulasi dalam format tabel."""

data_dir = 'Anime Recommendations Database'
anime = pd.read_csv(f"{data_dir}/anime.csv")
rating = pd.read_csv(f"{data_dir}/rating.csv")

"""Kode ini menampilkan 5 baris pertama dari DataFrame `anime` untuk memberikan gambaran awal terhadap struktur dan isi data."""

anime.head()

"""Kode ini menampilkan informasi struktur DataFrame `anime`, termasuk jumlah entri, nama kolom, jumlah nilai non-null, dan tipe data setiap kolom."""

anime.info()

"""Kode ini menghasilkan ringkasan statistik deskriptif untuk kolom numerik dalam DataFrame `anime`, seperti mean, standar deviasi, nilai minimum, maksimum, dan kuartil."""

anime.describe()

"""Kode ini mengembalikan tuple yang menunjukkan dimensi DataFrame `anime`, yaitu jumlah baris dan kolom."""

anime.shape

"""Menampilkan 5 baris pertama dari DataFrame `rating` untuk melihat isi awal dataset."""

rating.head()

"""Menampilkan informasi struktur DataFrame `rating`, termasuk jumlah baris, kolom, nilai non-null, dan tipe data."""

rating.info()

"""Memberikan statistik deskriptif untuk kolom numerik dalam DataFrame `rating` seperti rata-rata, nilai minimum, maksimum, dan distribusi kuartil."""

rating.describe()

"""Mengembalikan jumlah baris dan kolom dalam DataFrame `rating` dalam bentuk tuple."""

rating.shape

"""### Sampling

Mengambil sampel acak sebanyak 50.000 baris dari DataFrame `rating` untuk keperluan analisis atau pelatihan model, dengan `random_state=42` agar hasil acakan dapat direproduksi secara konsisten. Serta menampilkan 5 baris pertama dari DataFrame `rating_sample` untuk memverifikasi isi dari sampel acak yang telah diambil.
"""

rating_sample = rating.sample(n=50000, random_state=42)
rating_sample.head()

"""### Visualisasi Data

#### Visualisasi Distribusi Rating Anime
Kode ini membuat histogram distribusi nilai rating pada dataset anime. Data rating yang kosong dihapus terlebih dahulu dengan `dropna()`. Histogram dibuat dengan 20 bins dan garis KDE (Kernel Density Estimation) untuk menunjukkan pola sebaran secara halus. Visualisasi ini membantu memahami seberapa sering nilai rating tertentu muncul.
"""

plt.figure(figsize=(10,6))
sns.histplot(anime['rating'].dropna(), bins=20, kde=True)
plt.title('Distribusi Rating Anime')
plt.xlabel('Rating')
plt.ylabel('Jumlah Anime')
plt.show()

"""Visualisasi Distribusi Rating Anime ini menunjukkan distribusi rating anime berdasarkan data yang ada. Rating anime paling banyak berada di kisaran 6 hingga 7, yang terlihat dari batang histogram tertinggi pada rentang tersebut. Secara umum, bentuk distribusinya menyerupai lonceng (distribusi normal), yang berarti sebagian besar anime memiliki rating di tingkat menengah, sementara yang memiliki rating sangat rendah atau sangat tinggi jumlahnya lebih sedikit. Garis lengkung biru di atas histogram merupakan kurva KDE (Kernel Density Estimation) yang membantu memperjelas pola sebaran data secara halus. Visualisasi ini memberikan gambaran bahwa rating anime cenderung terpusat di nilai tengah dan jarang ada yang memiliki rating ekstrem.

#### Visualisasi 10 Anime dengan Jumlah Members Terbanyak
Kode ini menampilkan bar chart dari 10 anime terpopuler berdasarkan jumlah `members` (anggota yang menambahkan anime ke daftar mereka). Data diurutkan menurun dan dipilih 10 teratas. Visualisasi horizontal ini memudahkan membandingkan popularitas antar anime.
"""

top_anime = anime.sort_values(by='members', ascending=False).head(10)
plt.figure(figsize=(12,6))
sns.barplot(data=top_anime, x='members', y='name', palette='viridis')
plt.title('10 Anime dengan Jumlah Members Terbanyak')
plt.xlabel('Jumlah Members')
plt.ylabel('Nama Anime')
plt.show()

"""Visualisasi 10 Anime dengan Jumlah Members Terbanyak ini menunjukkan 10 anime dengan jumlah members terbanyak berdasarkan data yang tersedia. Pada sumbu horizontal (x) ditampilkan jumlah members, yaitu jumlah pengguna yang menambahkan anime tersebut ke daftar mereka (biasanya di platform seperti MyAnimeList). Sedangkan sumbu vertikal (y) menunjukkan nama-nama anime.

Anime dengan jumlah members terbanyak adalah "Death Note", diikuti oleh "Shingeki no Kyojin" dan "Sword Art Online". Ini menunjukkan bahwa anime-anime tersebut sangat populer dan memiliki banyak penggemar atau penonton yang tertarik untuk menontonnya atau sudah menontonnya.

Grafik ini menggunakan warna gradasi dari palet viridis, yang membantu membedakan tiap batang secara visual. Tampilan horizontal memudahkan pembacaan nama-nama anime yang relatif panjang.

#### Visualisasi Distribusi Rating oleh Pengguna
Kode ini membuat count plot (diagram batang) yang menunjukkan frekuensi masing-masing nilai rating yang diberikan oleh pengguna berdasarkan sampel 50.000 data. Grafik ini memberikan gambaran tentang kecenderungan pengguna dalam memberikan rating, misalnya apakah lebih sering memberi nilai tinggi atau rendah.
"""

plt.figure(figsize=(10,6))
sns.countplot(data=rating_sample, x='rating', palette='magma')
plt.title('Distribusi Rating oleh Pengguna (Sample 50K)')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""Visualisasi Distribusi Rating oleh Pengguna ini menunjukkan distribusi rating yang diberikan oleh pengguna terhadap anime berdasarkan sampel data sebanyak 50.000 entri. Sumbu horizontal (x) menunjukkan nilai rating (dari 1 hingga 10, dan ada satu nilai -1), sedangkan sumbu vertikal (y) menunjukkan jumlah pengguna yang memberikan rating tersebut.

Dari grafik terlihat bahwa:

* Rating 8 adalah yang paling sering diberikan oleh pengguna, diikuti oleh rating 7 dan 9, menandakan bahwa pengguna cenderung memberikan penilaian yang tinggi terhadap anime yang mereka tonton.
* Rating -1 juga memiliki frekuensi yang sangat tinggi. Biasanya nilai ini menunjukkan entri yang belum diberi rating secara eksplisit oleh pengguna dalam data mentah.
* Rating yang lebih rendah seperti 1 hingga 5 jarang diberikan, yang mengindikasikan bahwa pengguna cenderung tidak terlalu sering memberi penilaian buruk.

## Content-Based Filtering (CBF)

### Data Preparation Content-Based Filtering

#### Data Cleaning

Menghapus data rating yang bernilai `-1`, karena dianggap sebagai rating tidak valid atau tidak tersedia.
"""

rating = rating[rating['rating'] != -1]

"""Mengisi nilai kosong (`NaN`) di kolom `genre` dan `type` dengan `'Unknown'`, sedangkan nilai kosong di kolom `rating` diisi dengan nilai median agar tetap representatif secara statistik.

"""

anime['genre'] = anime['genre'].fillna('Unknown').str.lower().str.strip()
anime['type'] = anime['type'].fillna('Unknown').str.lower().str.strip()
anime['rating'] = anime['rating'].fillna(anime['rating'].median())

"""Mengonversi kolom `episodes` menjadi numerik. Nilai tak valid diubah menjadi NaN (`errors='coerce'`), lalu diisi dengan median dan dikonversi menjadi integer untuk konsistensi."""

anime['episodes'] = pd.to_numeric(anime['episodes'], errors='coerce')
anime['episodes'] = anime['episodes'].fillna(anime['episodes'].median())
anime['episodes'] = anime['episodes'].astype(int)

"""Memastikan bahwa kolom `episodes` dan `rating` tidak memiliki nilai kosong setelah proses pembersihan. Jika ada, akan menimbulkan error."""

assert not anime[['episodes', 'rating']].isnull().any().any()

"""Menormalkan seluruh kolom teks dengan mengubah isinya menjadi huruf kecil dan menghapus spasi di awal/akhir, untuk konsistensi data."""

text_columns = anime.select_dtypes(include='object').columns
for col in text_columns:
    anime[col] = anime[col].str.lower().str.strip()

"""Menghilangkan spasi di sekitar koma pada kolom `genre`, agar setiap genre dapat dipisahkan dengan jelas sebagai token tunggal."""

anime['genre'] = anime['genre'].str.replace(r'\s*,\s*', ',', regex=True)

"""Membuang baris dengan genre `'unknown'` agar hanya anime dengan informasi genre yang valid digunakan untuk analisis lebih lanjut."""

anime_cleaned = anime[~anime['genre'].str.contains('unknown', na=False)].copy()

"""#### Membuat Dataset untuk Sistem Rekomendasi

Membuat DataFrame baru khusus untuk sistem rekomendasi, hanya berisi ID, judul, dan genre, serta mengganti nama kolom agar lebih deskriptif dan konsisten.
"""

content_based = anime_cleaned[['anime_id', 'name', 'genre']].copy()
content_based.columns = ['anime_id', 'title', 'genres']

"""#### TF-IDF Vectorization untuk Genre

Kode ini mengubah data genre menjadi vektor numerik menggunakan metode TF-IDF (Term Frequency-Inverse Document Frequency). Token dipisahkan berdasarkan koma, sehingga setiap genre diperlakukan sebagai kata unik. Hasilnya adalah `tfidf_matrix`, representasi vektor dari genre tiap anime.
"""

tfidf = TfidfVectorizer(token_pattern=r'[^,]+', ngram_range=(1,2))
tfidf_matrix = tfidf.fit_transform(content_based['genres'])

"""#### Menghitung Cosine Similarity antar Anime

Menghitung **cosine similarity** antar anime berdasarkan genre yang telah diubah ke bentuk vektor TF-IDF. Hasilnya adalah matriks kesamaan, di mana setiap nilai merepresentasikan kemiripan antara dua anime.
"""

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

"""### Modeling Content-Based Filtering

#### Fungsi Rekomendasi Anime Berbasis Konten

Fungsi ini memberikan rekomendasi anime berdasarkan **kemiripan genre**.
Langkah-langkahnya:

* Mencari anime berdasarkan judul input (dikonversi ke huruf kecil dan di-trim).
* Jika tidak ditemukan, kembalikan `top_n` anime secara acak.
* Jika ditemukan, ambil indeks anime tersebut, lalu cari anime lain dengan nilai cosine similarity tertinggi.
* Mengembalikan `top_n` anime paling mirip beserta skornya.
"""

def recommend_anime_content_based(title, top_n=5):
    title_lower = title.lower().strip()
    idx = title_to_idx.get(title_lower)

    if idx is None:
        fallback = content_based['title'].sample(top_n, random_state=42).tolist()
        return f"Anime '{title}' tidak ditemukan. Mungkin kamu tertarik dengan: {fallback}"

    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
    indices = [i[0] for i in sim_scores]
    similarity_scores = [i[1] for i in sim_scores]

    results = content_based.iloc[indices].copy()
    results['similarity_score'] = similarity_scores
    return results[['title', 'genres', 'similarity_score']].sort_values(by='similarity_score', ascending=False)

"""#### Menampilkan Hasil Rekomendasi untuk "Naruto"

Memanggil fungsi rekomendasi dan mencetak anime yang paling mirip dengan "Naruto" berdasarkan genre-nya. Jika "Naruto" tidak ditemukan, akan ditampilkan rekomendasi acak.
"""

print(recommend_anime_content_based("Naruto"))

"""### Evaluasi Model Content-Based Filtering"""

def evaluate_cbf_genre_overlap_fixed(content_based, cosine_sim, top_n=5, num_samples=100):
    sample_anime = content_based.sample(num_samples, random_state=42)

    precision_list = []
    recall_list = []

    for _, row in sample_anime.iterrows():
        title = row['title']
        true_genres = set(row['genres'].split(','))

        recommended = recommend_anime_content_based(title, top_n=top_n)
        if isinstance(recommended, str):
            continue

        recommended['genres_set'] = recommended['genres'].apply(lambda g: set(g.split(',')))

        relevant_recommendations = recommended['genres_set'].apply(lambda g: len(true_genres & g) > 0).sum()
        precision = relevant_recommendations / top_n

        recommended_genres_union = set.union(*recommended['genres_set'])
        recall = len(recommended_genres_union & true_genres) / len(true_genres) if len(true_genres) > 0 else 0

        precision_list.append(precision)
        recall_list.append(recall)

    avg_precision = np.mean(precision_list)
    avg_recall = np.mean(recall_list)

    return avg_precision, avg_recall

precision, recall = evaluate_cbf_genre_overlap_fixed(content_based, cosine_sim, top_n=5, num_samples=100)
print(f"Precision@5: {precision:.2f}, Recall@5: {recall:.2f}")

"""## Collaborative Filtering

### Data Preparation Collaborative Filtering

#### Set Random Seed

Kode ini memastikan **reproducibility** atau hasil yang konsisten dalam setiap eksekusi dengan mengatur seed acak untuk `NumPy`, `TensorFlow`, dan modul `random`.
"""

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

"""#### Menghapus nilai NaN dan rating tidak valid"""

rating = rating.dropna(subset=['user_id', 'anime_id', 'rating'])
rating = rating[rating['rating'] > 0].reset_index(drop=True)

"""#### Encoding ID Pengguna dan Anime"""

user_encoder = LabelEncoder()
anime_encoder = LabelEncoder()

rating['user_index'] = user_encoder.fit_transform(rating['user_id'])
rating['anime_index'] = anime_encoder.fit_transform(rating['anime_id'])

num_users = rating['user_index'].nunique()
num_anime = rating['anime_index'].nunique()

"""#### Menyimpan Mapping ke File"""

mappings = {
    'user_to_encoded.pkl': dict(zip(rating['user_id'], rating['user_index'])),
    'anime_to_encoded.pkl': dict(zip(rating['anime_id'], rating['anime_index'])),
    'encoded_to_user.pkl': dict(zip(rating['user_index'], rating['user_id'])),
    'encoded_to_anime.pkl': dict(zip(rating['anime_index'], rating['anime_id']))
}

for filename, mapping in mappings.items():
    with open(filename, 'wb') as f:
        pickle.dump(mapping, f)

"""#### Normalisasi Rating"""

scaler = MinMaxScaler()
rating['norm_rating'] = scaler.fit_transform(rating[['rating']])

"""#### Persiapan Fitur dan Target"""

X = rating[['user_index', 'anime_index']].values
y = rating['norm_rating'].values

"""#### Mengacak Data"""

X, y = shuffle(X, y, random_state=SEED)

"""#### Membagi Data Training dan Validasi"""

x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)

"""#### Menampilkan Statistik Data"""

print(f"Total data: {len(X)}")
print(f"Train: {len(x_train)}, Validation: {len(x_val)}")
print(f"Unique users: {num_users}, Unique anime: {num_anime}")

"""### Modeling Collaborative Filtering

Neural Collaborative Filtering (NCF)

#### Custom Weighted Huber Loss

Fungsi loss ini memodifikasi `Huber Loss` dengan **pembobotan tambahan**. Rating di bawah `threshold` akan memiliki bobot lebih besar (`alpha`), berguna jika ingin lebih sensitif terhadap rating rendah (mis. outlier atau ketidakpuasan pengguna).
"""

def weighted_huber(delta=0.5, alpha=2.0, threshold=0.3):
    huber = Huber(delta=delta)
    def loss(y_true, y_pred):
        w = tf.where(y_true < threshold, alpha, 1.0)
        return tf.reduce_mean(w * huber(y_true, y_pred))
    return loss

"""#### Arsitektur Model Embedding

Model menggunakan dua **embedding layer** untuk pengguna dan anime, menggabungkannya, lalu melewatkan ke **fully connected layers**. Arsitektur ini meniru pendekatan matrix factorization dengan fleksibilitas deep learning. Regularisasi (`L2`) dan dropout digunakan untuk mencegah overfitting. Optimizer `Adam` digunakan dengan `Huber Loss`.
"""

embedding_dim = 64

user_input = Input(shape=(1,), name='user_input')
anime_input = Input(shape=(1,), name='anime_input')

user_embed = Embedding(input_dim=num_users, output_dim=embedding_dim, embeddings_regularizer=l2(1e-5))(user_input)
anime_embed = Embedding(input_dim=num_anime, output_dim=embedding_dim, embeddings_regularizer=l2(1e-5))(anime_input)

user_vec = Flatten()(user_embed)
anime_vec = Flatten()(anime_embed)

concat = Concatenate()([user_vec, anime_vec])

x = Dense(128, kernel_regularizer=l2(1e-4))(concat)
x = LeakyReLU()(x)
x = BatchNormalization()(x)
x = Dropout(0.4)(x)

x = Dense(64, kernel_regularizer=l2(1e-4))(x)
x = LeakyReLU()(x)
x = BatchNormalization()(x)
x = Dropout(0.3)(x)

x = Dense(32, kernel_regularizer=l2(1e-4))(x)
x = LeakyReLU()(x)
x = Dropout(0.2)(x)

output = Dense(1)(x)

model = Model(inputs=[user_input, anime_input], outputs=output)
model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss=weighted_huber(delta=0.5, alpha=2.0, threshold=0.3),
    metrics=['mae']
)

model.summary()

"""#### Early Stopping & Learning Rate Scheduler

Callback `early_stop` menghentikan pelatihan jika **val\_loss tidak membaik selama 5 epoch**. `reduce_lr` akan menurunkan learning rate jika val\_loss stagnan selama 3 epoch. Keduanya meningkatkan efisiensi dan performa pelatihan.
"""

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)

"""#### Pelatihan Model

Melatih model dengan input pasangan user-anime dan target rating, selama maksimal 50 epoch. Ukuran batch ditetapkan 128, dan callback digunakan untuk mengatur pelatihan secara dinamis. Hasil pelatihan dapat dipantau melalui `history`.
"""

history = model.fit(
    [x_train[:, 0], x_train[:, 1]], y_train,
    validation_data=([x_val[:, 0], x_val[:, 1]], y_val),
    epochs=50,
    batch_size=128,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

"""### Evaluasi Model Collaborative Filtering

#### Prediksi dan Transformasi Skala
* `model.predict(...)`: Melakukan prediksi rating dari data validasi berdasarkan pasangan (user, anime).
* `scaler.inverse_transform(...)`: Mengembalikan nilai rating ke **skala asli** (biasanya 1–10) karena sebelumnya telah dinormalisasi.
* `flatten()`: Mengubah array 2D menjadi 1D untuk memudahkan evaluasi dan visualisasi.
"""

y_pred = model.predict([x_val[:, 0], x_val[:, 1]]).flatten()
y_true = scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()
y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()

"""#### Evaluasi Kinerja Model (MSE, RMSE, MAE)
Menghitung metrik evaluasi prediksi:

* **MSE (Mean Squared Error)**: Rata-rata kuadrat selisih antara nilai aktual dan prediksi.
* **RMSE (Root Mean Squared Error)**: Akar dari MSE, lebih mudah dipahami karena satuannya sama dengan rating.
* **MAE (Mean Absolute Error)**: Rata-rata selisih absolut antara nilai aktual dan prediksi.

Metrik ini digunakan untuk mengukur **akurasi model dalam memprediksi rating pengguna** terhadap anime.

"""

mse = mean_squared_error(y_true, y_pred_rescaled)
mae = mean_absolute_error(y_true, y_pred_rescaled)
rmse = np.sqrt(mse)

print(f"MSE : {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE : {mae:.4f}")

"""#### Visualisasi Loss Selama Training
Plot ini memperlihatkan perkembangan **loss** (kerugian) model selama proses pelatihan.

* `Train Loss`: Loss pada data pelatihan.
* `Val Loss`: Loss pada data validasi.

Tujuannya adalah untuk memantau apakah model mengalami **overfitting** atau **underfitting**.

"""

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epoch')
plt.legend()

"""Visualisasi Loss Selama Training ini menunjukkan perubahan nilai loss selama proses pelatihan model (training) dari epoch ke epoch. Garis biru mewakili train loss (kesalahan pada data latih), dan garis oranye menunjukkan val loss (kesalahan pada data validasi).

Terlihat bahwa pada awal pelatihan (epoch 0 ke 1), nilai train loss turun drastis, lalu mulai stabil setelahnya. Nilai val loss juga cukup stabil dan rendah sejak awal. Ini menandakan bahwa model belajar dengan cepat di awal dan kemudian mampu menjaga performa yang baik tanpa overfitting. Karena nilai train loss dan val loss sama-sama kecil dan mendekati, ini menunjukkan bahwa model bekerja dengan baik dan konsisten pada data pelatihan maupun data validasi.

#### Visualisasi MAE Selama Training
Plot ini memperlihatkan **MAE (Mean Absolute Error)** per epoch pada data pelatihan dan validasi.

* Membantu memahami apakah **prediksi semakin membaik** seiring bertambahnya epoch.
* Disandingkan dengan plot loss agar bisa dilihat konsistensi performa model.
"""

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Val MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.title('MAE vs Epoch')
plt.legend()

plt.tight_layout()
plt.show()

"""Visualisasi MAE Selama Training ini menunjukkan perubahan nilai MAE (Mean Absolute Error) selama proses pelatihan model. Garis biru menunjukkan train MAE (kesalahan rata-rata pada data latih), sedangkan garis oranye menunjukkan val MAE (kesalahan rata-rata pada data validasi).

Dari grafik terlihat bahwa MAE menurun tajam di awal (dari epoch 0 ke 1), lalu terus menurun secara perlahan dan stabil seiring bertambahnya epoch. Nilai train MAE dan val MAE saling berdekatan dan sama-sama rendah, yang berarti model belajar dengan baik dan konsisten, serta tidak mengalami overfitting. Kesalahan prediksi model semakin kecil dari waktu ke waktu, baik pada data latih maupun validasi. Ini menandakan bahwa model cukup akurat dan dapat diandalkan.

#### Visualisasi Scatter Plot Prediksi vs Aktual
Visualisasi ini menunjukkan seberapa **dekat prediksi model** terhadap nilai aktual:

* Titik-titik di **garis merah putus-putus (Perfect Prediction)** menunjukkan prediksi sempurna.
* Penyebaran titik menggambarkan akurasi dan **deviasi kesalahan model**.

Cocok untuk memvisualisasikan model regresi seperti ini dalam sistem rekomendasi.
"""

plt.figure(figsize=(6, 6))
plt.scatter(y_true, y_pred_rescaled, alpha=0.5, label='Predicted Ratings')
plt.plot([0, 10], [0, 10], 'r--', label='Perfect Prediction')
plt.xlabel('Actual Rating')
plt.ylabel('Predicted Rating')
plt.title('Actual vs Predicted Ratings')
plt.legend()
plt.grid(True)
plt.show()

"""Visualisasi Actual vs Predicted Ratings ini menunjukkan seberapa akurat model dalam memprediksi rating dibandingkan dengan nilai rating yang sebenarnya. Setiap titik biru pada grafik mewakili satu data, misalnya satu ulasan dari pengguna, dengan sumbu horizontal menunjukkan rating yang sebenarnya (Actual Rating), dan sumbu vertikal menunjukkan rating yang diprediksi oleh model (Predicted Rating).

Garis merah putus-putus adalah garis prediksi sempurna, di mana nilai prediksi sama persis dengan nilai aktual. Jika model sangat akurat, maka semua titik akan berada di sepanjang garis ini. Namun, dari grafik terlihat bahwa banyak titik berada di atas garis merah ketika nilai aktual rendah, artinya model sering memberikan rating yang lebih tinggi dari sebenarnya (overestimate). Sebaliknya, saat nilai aktual tinggi, banyak titik berada di bawah garis, menunjukkan model sering memprediksi lebih rendah dari nilai sebenarnya (underestimate).

Sebagai contoh, ketika nilai rating sebenarnya adalah 2, banyak prediksi model mendekati 7 atau lebih, dan ketika rating sebenarnya adalah 9 atau 10, model kadang memprediksi hanya 6 atau 7. Ini menunjukkan bahwa model belum cukup akurat dan cenderung bias ke arah nilai tengah. Untuk meningkatkan akurasi, model dapat ditingkatkan dengan teknik tuning atau menggunakan data pelatihan yang lebih seimbang.
"""